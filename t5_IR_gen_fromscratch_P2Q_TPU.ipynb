{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5-IR-gen-fromscratch-P2Q_TPU",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DylanJoo/docTTTTTquery/blob/master/t5_IR_gen_fromscratch_P2Q_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arkjElSVKSp4",
        "colab_type": "text"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3NU4w_EKAQQ",
        "colab_type": "code",
        "outputId": "98364305-8ef5-4501-e48d-7ceef2085d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 15.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 57.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=2712123d14916029939c9a1017549a505434ae72546e01518f06bac1a048cde6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GepRmRBIsD1E",
        "colab_type": "text"
      },
      "source": [
        "# Install/Check TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DoWWb4ANm2t",
        "colab_type": "code",
        "outputId": "9d53e5ff-1ba2-4cc0-93cb-5faea244934f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "# Installs PyTorch, PyTorch/XLA, and Torchvision\n",
        "# Copy this cell into your own notebooks to use PyTorch on Cloud TPUs \n",
        "# Warning: this may take a couple minutes to run\n",
        "\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4264  100  4264    0     0  50761      0 --:--:-- --:--:-- --:--:-- 50761\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-nightly ...\n",
            "Uninstalling torch-1.5.0+cu101:\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "  Successfully uninstalled torch-1.5.0+cu101\n",
            "Uninstalling torchvision-0.6.0+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 89.5 MiB/ 89.5 MiB]                                                \n",
            "Operation completed over 1 objects/89.5 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][116.9 MiB/116.9 MiB]                                                \n",
            "Operation completed over 1 objects/116.9 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  1.7 MiB/  1.7 MiB]                                                \n",
            "Operation completed over 1 objects/1.7 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (0.16.0)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+a25b1b9\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+407518\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.6.0a0+a25b1b9)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+3e06bc6\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (379 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144467 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AgjzonHgtom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZTpRnkvRord",
        "colab_type": "text"
      },
      "source": [
        "# Mounting MSMARCO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kihxoBeiVw4e",
        "colab_type": "code",
        "outputId": "4c373b9f-d1be-4257-bfb1-3fd4b354c10e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn4_HWQLWpEV",
        "colab_type": "text"
      },
      "source": [
        "# Args"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqo9mR5MK5rW",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "DATA_DIR = \"/content/drive/My Drive/Colab Notebooks/msmarco_data/\" #@param {type:\"string\"}\n",
        "BATCH_SIZE =  32#@param {type:\"integer\"}\n",
        "EPOCHS =  7#@param {type:\"integer\"}\n",
        "DEVICE = \"xla\" #@param {type:\"string\"}\n",
        "LEARNING_RATE = 0.01 #@param {type:\"number\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iAcG1MfTiOw",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing: Building Vocabulary\n",
        "  - Build SP model via Python: ALL text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oex9D0vEQ2-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import T5Tokenizer\n",
        "import sentencepiece as spm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5lP6bFFle8A",
        "colab_type": "code",
        "outputId": "04fa100c-75c0-44cb-cd3f-71d31ae4d244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Quick test\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/msmarco_data\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('msmarco.model')\n",
        "\n",
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
        "\n",
        "for id in range(4):\n",
        "  print(sp.id_to_piece(id), sp.is_control(id))\n",
        "  \n",
        "%cd /"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/msmarco_data\n",
            "['▁This', '▁is', '▁a', '▁test']\n",
            "This is a test\n",
            "<pad> True\n",
            "<unk> False\n",
            "<s> True\n",
            "</s> True\n",
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iebn9JRJNNIW",
        "colab_type": "code",
        "outputId": "47c22400-1be2-4ddd-cbf2-2d2fdecbc531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/msmarco_data\n",
        "T5tokenizer = T5Tokenizer(vocab_file='msmarco.model', bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', extra_ids=100)\n",
        "\n",
        "# Get tokenizer pretrained meatadata\n",
        "print(\"Vocab size: \", T5tokenizer.vocab_size)\n",
        "print(\"Specs: {} {} {} {}\".format( T5tokenizer.pad_token_id, T5tokenizer.unk_token_id, T5tokenizer.bos_token_id, T5tokenizer.eos_token_id))\n",
        "print(\"Vocab SentencePiece:\", \"/\".join(list(T5tokenizer.get_vocab())[9527:9537]))\n",
        "\n",
        "# Quick test\n",
        "print(T5tokenizer.tokenize('hello this is a test'))\n",
        "print(T5tokenizer.encode('hello this is a test', add_special_tokens=True))\n",
        "\n",
        "# Add special token\n",
        "specs = {'bos_token': '<s>', \n",
        "         'eos_token':'</s>',\n",
        "         'unk_token':'<unk>',\n",
        "         'pad_token':'<pad>'}\n",
        "T5tokenizer.add_special_tokens(specs)\n",
        "\n",
        "# Test\n",
        "T5tokenizer.encode('<s> Hello, <unk> this is a test <pad> </s>')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/msmarco_data\n",
            "Vocab size:  48100\n",
            "Specs: 0 1 2 3\n",
            "Vocab SentencePiece: ▁altered/▁-8/▁Economic/▁charter/NP/▁resembles/▁premiums/▁HDL/▁learners/▁logical\n",
            "['▁hello', '▁this', '▁is', '▁a', '▁test']\n",
            "[25887, 54, 8, 9, 257]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 14286, 6, 1, 54, 8, 9, 257, 0, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sNl3tTdkFj-x"
      },
      "source": [
        "# Preprocessing: Tokenizing/numericalizing pipeline\n",
        "  - With hug-face tokenizer API pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M36qSqU4IMkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
        "\n",
        "## Preprocess for dataloader \n",
        "def preprocess4loader(data_dir, file_src, file_tgt, file_pkl, tokenizer, src_prefix=None, tgt_prefix=None, src_infix=None):\n",
        "    \"\"\"for seq2seq so far\"\"\"\n",
        "    src_prefix = src_prefix if src_prefix else \"\"\n",
        "    src_infix = src_infix if src_infix else \"\"\n",
        "    tgt_prefix = tgt_prefix if tgt_prefix else \"\"\n",
        "\n",
        "    bos_index = tokenizer.convert_tokens_to_ids(tokenizer.bos_token)\n",
        "    eos_index = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
        "    unk_index = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "    pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "\n",
        "    data = dict()\n",
        "    data['src'] = list()\n",
        "    data['tgt'] = list()\n",
        "\n",
        "    with open(data_dir + file_src, 'r') as src_txt, open(data_dir + file_tgt, 'r') as tgt_txt:\n",
        "        for i, (line1, line2) in enumerate(zip(src_txt, tgt_txt)):\n",
        "            src_sent = tokenizer.encode(src_prefix + line1 + tokenizer.eos_token, max_length=256, pad_to_max_length=True)\n",
        "            tgt_sent = tokenizer.encode(tgt_prefix + line2 + tokenizer.eos_token, max_length=64, pad_to_max_length=True)\n",
        "            data['src'].append(src_sent)\n",
        "            data['tgt'].append(tgt_sent)\n",
        "\n",
        "    # For accelerate IO process, save into pickle file in adcance.\n",
        "    with open(data_dir + file_pkl, 'wb') as pkl:\n",
        "        pickle.dump(data, pkl)\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, file_pkl):\n",
        "        self.data = dict()\n",
        "        self._load_pickle(data_dir + file_pkl)\n",
        "\n",
        "    def _load_pickle(self, path):\n",
        "        with open(path, 'rb') as file:\n",
        "            self.data = pickle.load(file)\n",
        "        print(\"Data Loaded.\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Returns one data pair (source and target).\"\"\"\n",
        "        src = self.data['src'][index]\n",
        "        tgt = self.data['tgt'][index]\n",
        "        return src, tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['src'])\n",
        "\n",
        "def getDataloader(data_dir, file_pkl, batch_size=BATCH_SIZE, device=DEVICE, sort=False, shuffle=False):\n",
        "\n",
        "    # Build into customized dataset\n",
        "    dataset = MyDataset(data_dir, file_pkl)\n",
        "\n",
        "    # Sampler\n",
        "    if xm.xrt_world_size() <= 1:\n",
        "        sampler = RandomSampler(dataset)\n",
        "    else:\n",
        "        sampler = DistributedSampler(\n",
        "            dataset=dataset, \n",
        "            num_replicas=xm.xrt_world_size(), \n",
        "            rank=xm.get_ordinal,\n",
        "            shuffle=False)\n",
        "        \n",
        "    # Dataloader (Global)\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=sampler,\n",
        "        shuffle=False if sampler else True,\n",
        "        num_workers=8)\n",
        "    \n",
        "    # Dataloader (distributed/parallel)\n",
        "    loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n",
        "\n",
        "    return loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "borO3meX9vZo",
        "colab_type": "text"
      },
      "source": [
        "# Text to Text: Task building\n",
        "- Setting tasks\n",
        "  - (Main) D2Q: Generation\n",
        "  - (Sub) Q2D: Generation\n",
        "  - (Clf) Relevance: Similarity scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uux9hzYJ7M0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "### Token type batching Required(dynamically batching)###\n",
        "def max_tok_len(new, count, sofar):\n",
        "    global max_src_in_batch, max_tgt_in_batch  # this is a hack\n",
        "    # Reset current longest length at a new batch (count=1)\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    # Src: [w1 ... wN]\n",
        "    max_src_in_batch = max(max_src_in_batch, len(new.src))\n",
        "    # Tgt: [<bos> w1 ... wM <eos>]\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch, len(new.tgt) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vex3gg8A9vg9",
        "colab_type": "code",
        "outputId": "97630bc2-6c3f-449f-fe23-a30d30acd491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# P2Q\n",
        "'''\n",
        "# Training\n",
        "[src]: \"From Passage to Query: <!--Passage-here--> \"\n",
        "[tgt]: \"<!--Query-here--.>\"\n",
        "# Inferencing\n",
        "[source]: \"From Passage to Query: <!--Passage-here--> Target:\"\n",
        "'''\n",
        "# Preprocessing: Includes Tokenizing, Encoding .... then pickle the dataset\n",
        "#preprocess(DATA_DIR, 'src-train.txt', 'tgt-train.txt', 'p2q-train.pkl', T5tokenizer, False, 'From Passage to Query: ')\n",
        "#preprocess(DATA_DIR, 'src-valid.txt', 'tgt-valid.txt', 'p2q-valid.pkl', T5tokenizer, False, 'From Passage to Query: ')\n",
        "\n",
        "# Get the training iterators\n",
        "#data['train']['P2Q'] = getIterator(DATA_DIR, 'p2q-train.pkl', BATCH_SIZE, DEVICE,  True)\n",
        "#data['valid']['P2Q'] = getIterator(DATA_DIR, 'p2q-valid.pkl', BATCH_SIZE, DEVICE, False)\n",
        "\n",
        "## For TPU, in dataloader \n",
        "#preprocess4loader(DATA_DIR, 'src-train.txt', 'tgt-train.txt', 'p2q-train-loader.pkl', T5tokenizer, 'From Passage to Query: ')\n",
        "#preprocess4loader(DATA_DIR, 'src-valid.txt', 'tgt-valid.txt', 'p2q-valid-loader.pkl', T5tokenizer, 'From Passage to Query: ')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Training\\n[src]: \"From Passage to Query: <!--Passage-here--> \"\\n[tgt]: \"<!--Query-here--.>\"\\n# Inferencing\\n[source]: \"From Passage to Query: <!--Passage-here--> Target:\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDZNnb-uFyAd",
        "colab_type": "code",
        "outputId": "6eb42a7d-7747-4799-b00c-eca378a3507f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# IR Ranking\n",
        "'''\n",
        "# Training\n",
        "[src]: \"Ranking: document1: <!--Passage-here--> document2: <!--Query-here-->\"\n",
        "[tgt]: \"positive\"\n",
        "p.s.: Given 1 to 5? Gold from the BM25 baseline.\n",
        "'''\n",
        "######################################################\n",
        "# Shuffling the query to generate the negative sample\n",
        "#with open(DATA_DIR + 'tgt-train.txt', 'r') as rand_tgt:\n",
        "#    rand_tgt = rand_tgt.readlines()\n",
        "#    random.shuffle(rand_tgt)\n",
        "#    with open(DATA_DIR + 'Rtgt-train.txt', 'w') as new_tgt:\n",
        "#        new_tgt.writelines(rand_tgt)\n",
        "#####################################################\n",
        "\n",
        "#preprocess_ir(DATA_DIR, 'src-train.txt', 'tgt-train.txt', 'ir-train.pkl', T5tokenizer, False, 'Ranking document1: ', ' document2 ', isTrain=True)\n",
        "#preprocess_ir(DATA_DIR, 'src-valid.txt', 'tgt-valid.txt', 'ir-valid.pkl', T5tokenizer, False, 'Ranking document1: ', ' document2 ', isTrain=False)\n",
        "\n",
        "# Get the training iterators\n",
        "#data['train']['IR-CLF'] = getIterator(DATA_DIR, 'ir-train.pkl', BATCH_SIZE, DEVICE, True, shuffle=True)\n",
        "#data['valid']['IR-CLF'] = getIterator(DATA_DIR, 'ir-valid.pkl', BATCH_SIZE, DEVICE, False)\n",
        "\n",
        "# For TPU use\n",
        "#preprocess4loader(DATA_DIR, 'src-train.txt', 'tgt-train.txt', 'p2q-train-loader.pkl', T5tokenizer, 'From Passage to Query: ')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Training\\n[src]: \"Ranking: document1: <!--Passage-here--> document2: <!--Query-here-->\"\\n[tgt]: \"positive\"\\np.s.: Given 1 to 5? Gold from the BM25 baseline.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV7KrBLR_Zrb",
        "colab_type": "text"
      },
      "source": [
        "# Utillities\n",
        "\n",
        "- tok2word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMjtwh_5NhwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Input a array of index, each row represent a sentence(by SP). '''\n",
        "def tok2word(batch):\n",
        "  batch_sent = []\n",
        "  for sent in batch:\n",
        "    samples = T5tokenizer.decode(sent, skip_special_tokens=True)\n",
        "    batch_sent.append(samples)\n",
        "  return batch_sent\n",
        "\n",
        "def tgt2gold(tgt, pad_idx=0):\n",
        "    batch_size = tgt.size(0)\n",
        "    pads = torch.LongTensor([[pad_idx]]*batch_size).to(DEVICE)\n",
        "    return torch.cat((tgt[:, 1:], pads), dim=-1)\n",
        "\n",
        "def sent_dump(data_dir, file_output, samples):\n",
        "    with open(data_dir+'/results/'+file_output, 'w') as file:\n",
        "        file.writelines(samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm-7pbbEpayJ",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture\n",
        "- T5 for enc-der\n",
        "- tok2vocab\n",
        "- Loss\n",
        "\n",
        "- T5 for CondtionalGen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WTK-WtYqR2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import T5Config, T5ForConditionalGeneration\n",
        "# Setup configuration\n",
        "config = T5Config(\n",
        "    vocab_size=T5tokenizer.vocab_size,\n",
        "    n_position=512, \n",
        "    d_model=768, \n",
        "    d_kv=64,\n",
        "    d_ff=3072,\n",
        "    num_layers=12,\n",
        "    num_heads=12,\n",
        "    relative_attention_num_buckets=32,\n",
        "    dropout_rate=0.1,\n",
        "    layer_norm_epsilon=1e-6,\n",
        "    pad_token_id=T5tokenizer.pad_token_id,\n",
        "    unk_token_id=T5tokenizer.unk_token_id,\n",
        "    bos_token_id=T5tokenizer.bos_token_id,\n",
        "    eos_token_id=T5tokenizer.eos_token_id,\n",
        "    decoder_start_token_id=T5tokenizer.bos_token_id\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1ODn_EiCtfF",
        "colab_type": "code",
        "outputId": "46d8cda1-6b0e-4299-cadb-77bafb931724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class T5_gen(nn.Module):\n",
        "\n",
        "    def __init__(self, conf):\n",
        "        super().__init__()\n",
        "        self.config = conf\n",
        "        self.t5 = T5ForConditionalGeneration(conf)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
        "        self.bos = conf.bos_token_id\n",
        "        self.max_len = 20\n",
        "\n",
        "    def forward(self, src_seq, tgt_seq, selfmask=None, crossmask=None, beam_size=None, num_out=1, do_sample=True):\n",
        "\n",
        "        if tgt_seq is not None:\n",
        "            loss, output, _, _ =  self.t5(input_ids=src_seq,\n",
        "                                     attention_mask=selfmask,\n",
        "                                     lm_labels=tgt_seq,\n",
        "                                     decoder_attention_mask=crossmask, #encoder_output=XXX, scenario for decoding only task.\n",
        "                                     head_mask=None)\n",
        "            logits = self.logsoftmax(output)\n",
        "            sample = torch.argmax(logits.detach(), dim=-1) # Set no gradients.\n",
        "\n",
        "            return logits, sample, loss\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                output = self.t5.generate(input_ids=src_seq, \n",
        "                                      max_length=64,\n",
        "                                      num_beams=1 if do_sample else beam_size, \n",
        "                                      do_sample=do_sample,\n",
        "                                      top_k=10 if do_sample else None,\n",
        "                                      num_return_sequences=num_out)\n",
        "            return output\n",
        "\n",
        "    def forward_clf(self, src_seq, tgt_seq, selfmask=None, crossmask=None):\n",
        "        loss, output, _, _ = self.t5(input_ids=src_seq,\n",
        "                                    attention_mask=selfmask,\n",
        "                                    lm_labels=tgt_seq, \n",
        "                                    decoder_attention_mask=crossmask, #encoder_output=XXX, scenario for decoding only task.\n",
        "                                    head_mask=None)\n",
        "        logits = self.logsoftmax(output[:, 0, :]) # B, VS\n",
        "        sample = torch.argmax(logits.detach(), dim=-1) # Set no gradients.\n",
        "\n",
        "        return logits, sample, loss\n",
        "\n",
        "    def inference(self, src_seq, selfmask=None, crossmask=None, device='cpu'):\n",
        "        n_batch = src_seq.size(0)\n",
        "        # Augmented fake trg & Preocessed\n",
        "        pads = torch.zeros((n_batch, self.max_len)).long()\n",
        "        generations = torch.cat((torch.LongTensor([[self.bos]]*n_batch), pads[:, :-1]), dim=-1) #pseduo generations            \n",
        "        \n",
        "        logits = []\n",
        "        # Loop the inference\n",
        "        for i in range(1, self.max_len):\n",
        "            print('s')\n",
        "\n",
        "            tgt_seq = generations[:, :i]\n",
        "            loss, output, _, _ = self.t5(input_ids=src_seq,\n",
        "                                        attention_mask=selfmask,\n",
        "                                        decoder_input_ids=tgt_seq,\n",
        "                                        decoder_attention_mask=crossmask, #encoder_output=XXX, scenario for decoding only task.\n",
        "                                        head_mask=None)\n",
        "            print('e')\n",
        "                \n",
        "            logit = self.logsoftmax(self.final(output))\n",
        "            generations[:, i] = torch.argmax(logit[:, -1, :], dim=-1)\n",
        "            logits.append(logit[:, -1, :])\n",
        "\n",
        "        logits = torch.stack(logits, dim=1)\n",
        "        logits = torch.cat((logits, torch.tensor(n_batch, 1, logits.size(2))), dim=1)\n",
        "\n",
        "        return logits, generations[:, 1:]\n",
        "    \n",
        "    def inference2(self, src_seq, beam_size=1, num_out=1, do_sample=False, selfmask=None, crossmask=None):\n",
        "        n_batch = src_seq.size(0)\n",
        "        output = self.t5.generate(input_ids=src_seq, \n",
        "                                    max_length=64,\n",
        "                                    num_beams=1 if do_sample else beam_size, \n",
        "                                    do_sample=do_sample,\n",
        "                                    top_k=10 if do_sample else None,\n",
        "                                    num_return_sequences=num_out)\n",
        "        return output\n",
        "\n",
        "# Quick test: FP\n",
        "print(T5tokenizer.encode('true', return_tensors='pt'))\n",
        "print(T5tokenizer.encode('false', return_tensors='pt'))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1363]])\n",
            "tensor([[4630]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfnNKJorC4If",
        "colab_type": "text"
      },
      "source": [
        "## AdaFactorOptimizer 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcATo0BCC3tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "import functools\n",
        "from copy import copy\n",
        "from math import sqrt\n",
        "\n",
        "class AdaFactor(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=None, beta1=0.9, beta2=0.999, eps1=1e-30, \n",
        "                 eps2=1e-3, cliping_threshold=1,non_constant_decay = True,\n",
        "                 enable_factorization=True, ams_grad=True, weight_decay=0):\n",
        "        \n",
        "        enable_momentum =  beta1 != 0\n",
        "        self.beta1_glob = copy(beta1)\n",
        "        self.beta2_glob = copy(beta2)\n",
        "        self.lr_glob = copy(lr)\n",
        "        \n",
        "        beta1 = self.beta1_glob if hasattr(beta1,'__call__') else lambda x: self.beta1_glob\n",
        "        beta2 = self.beta2_glob if hasattr(beta2,'__call__') else lambda x: self.beta2_glob\n",
        "\n",
        "        if non_constant_decay:\n",
        "            ams_grad = False\n",
        "            if isinstance(self.beta1_glob,float):\n",
        "                beta1 = lambda t: self.beta1_glob * (1 - self.beta1_glob ** (t-1)) / (1 - self.beta1_glob ** t)\n",
        "            if isinstance(self.beta2_glob,float):\n",
        "                beta2 = lambda t: self.beta2_glob * (1 - self.beta2_glob ** (t-1)) / (1 - self.beta2_glob ** t)\n",
        "\n",
        "        relative_step_size  = True\n",
        "        \n",
        "        if lr is None:\n",
        "            #default value from article\n",
        "            lr = lambda t: min(1e-2, 1 / sqrt(t))\n",
        "            \n",
        "        if isinstance(self.lr_glob, float):\n",
        "            lr=lambda x: self.lr_glob\n",
        "            relative_step_size = False\n",
        "  \n",
        "                         \n",
        "        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps1=eps1,\n",
        "                        eps2=eps2, cliping_threshold=cliping_threshold,\n",
        "                        weight_decay=weight_decay,ams_grad=ams_grad,\n",
        "                        enable_factorization=enable_factorization,\n",
        "                        enable_momentum=enable_momentum,relative_step_size=relative_step_size)\n",
        "        \n",
        "        super(AdaFactor, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AdaFactor, self).__setstate__(state)       \n",
        "     \n",
        "    def _experimental_reshape(self,shape):\n",
        "        temp_shape = shape[2:]\n",
        "        if len(temp_shape) == 1:\n",
        "            new_shape = (shape[0],shape[1]*shape[2])\n",
        "        else:\n",
        "            tmp_div = len(temp_shape) // 2 + len(temp_shape) % 2           \n",
        "            new_shape = (shape[0]*functools.reduce(operator.mul, temp_shape[tmp_div:],1),\n",
        "                         shape[1]*functools.reduce(operator.mul, temp_shape[:tmp_div],1))\n",
        "        return new_shape, copy(shape)\n",
        "        \n",
        "        \n",
        "    def _check_shape(self, shape):\n",
        "        '''\n",
        "        output1 - True - algorithm for matrix, False - vector;\n",
        "        output2 - need reshape\n",
        "        '''\n",
        "        if len(shape) > 2:\n",
        "            return True, True\n",
        "        elif len(shape) == 2:\n",
        "            return True, False\n",
        "        elif len(shape) == 2 and (shape[0] == 1 or shape[1] == 1):\n",
        "            return False, False\n",
        "        else:\n",
        "            return False, False\n",
        "        \n",
        "    def _rms(self, x):\n",
        "        return sqrt(torch.mean(x.pow(2)))\n",
        "    \n",
        "    \n",
        "    \n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()       \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                data_backup = p.data.clone().detach()\n",
        "\n",
        "                    \n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead') \n",
        "                    \n",
        "                is_matrix, is_need_reshape = self._check_shape(grad.size())\n",
        "                new_shape = p.data.size()\n",
        "                if is_need_reshape and group['enable_factorization']:\n",
        "                    new_shape, old_shape =\\\n",
        "                    self._experimental_reshape(p.data.size())\n",
        "                    grad = grad.view(new_shape)\n",
        "               \n",
        "                state = self.state[p]\n",
        "                grad_shape = grad.shape\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    if group['enable_momentum']:\n",
        "                        state['exp_avg'] = torch.zeros_like(grad)\n",
        "\n",
        "                    if is_matrix and group['enable_factorization']:\n",
        "                        state['exp_avg_sq_R'] = torch.zeros(grad_shape[:-1]).to(grad)\n",
        "                        state['exp_avg_sq_C'] = torch.zeros(grad_shape[:-2] + grad_shape[-1:]).to(grad)\n",
        "                    else:\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(grad)\n",
        "                    if group['ams_grad']:\n",
        "                        state['exp_avg_sq_hat'] = torch.zeros(new_shape).to(grad)\n",
        "\n",
        "                if group['enable_momentum']:\n",
        "                    exp_avg = state['exp_avg']\n",
        "                    \n",
        "                if is_matrix and group['enable_factorization']:\n",
        "                    exp_avg_sq_R = state['exp_avg_sq_R'].to(grad)\n",
        "                    exp_avg_sq_C = state['exp_avg_sq_C'].to(grad)\n",
        "                else:\n",
        "                    exp_avg_sq = state['exp_avg_sq'].to(grad)\n",
        "                \n",
        "                if group['ams_grad']:\n",
        "                    exp_avg_sq_hat = state['exp_avg_sq_hat'].to(grad)\n",
        "                \n",
        "                \n",
        "                state['step'] += 1\n",
        "                lr_t = group['lr'](state['step'])\n",
        "                if group['relative_step_size']:\n",
        "                    lr_t *= max(group['eps2'], self._rms(p.data))\n",
        "                          \n",
        "                if group['enable_momentum']:\n",
        "                    beta1_t = group['beta1'](state['step'])\n",
        "                    exp_avg.mul_(beta1_t).add_(1 - beta1_t, grad)\n",
        "                    \n",
        "                beta2_t = group['beta2'](state['step']) \n",
        "\n",
        "                if is_matrix and group['enable_factorization']:\n",
        "                    exp_avg_sq_R.mul_(beta2_t).add_(1 - beta2_t,                   \n",
        "                      torch.sum(torch.mul(grad,grad).add_(group['eps1']), dim=0, keepdim=True))\n",
        "                    exp_avg_sq_C.mul_(beta2_t).add_(1 - beta2_t,                   \n",
        "                      torch.sum(torch.mul(grad,grad).add_(group['eps1']), dim=1, keepdim=True))\n",
        "                    v = torch.mul(exp_avg_sq_C,exp_avg_sq_R).div_(torch.sum(exp_avg_sq_R))\n",
        "                else:\n",
        "                    exp_avg_sq.mul_(beta2_t).addcmul_(1 - beta2_t, grad, grad).add_((1 - beta2_t)*group['eps1'])\n",
        "                    v = exp_avg_sq\n",
        "\n",
        "                \n",
        "                g = grad\n",
        "                if group['enable_momentum']:\n",
        "                    g = torch.div(exp_avg,1 - beta1_t ** state['step'])\n",
        "                               \n",
        "                if group['ams_grad']:\n",
        "                    torch.max(exp_avg_sq_hat, v, out=exp_avg_sq_hat)\n",
        "                    v = exp_avg_sq_hat                    \n",
        "                    u = torch.div(g,(torch.div(v,1 - beta2_t ** state['step'])).sqrt().add_(group['eps1']))\n",
        "                else:\n",
        "                    u = torch.div(g,v.sqrt()) \n",
        "       \n",
        "                u.div_((self._rms(update) / group['clip_threshold']).clamp_(min=1.0))\n",
        "                p.data.add_(-lr_t * (u.view(old_shape) if is_need_reshape and group['enable_factorization'] else u))\n",
        "                \n",
        "                if group['weight_decay'] != 0:\n",
        "                    p.data.add_(-group['weight_decay'] * lr_t, data_backup)\n",
        "                    \n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnjQD7yMnwWz",
        "colab_type": "text"
      },
      "source": [
        "# AdaFactorOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YksvVvBjIac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.optim\n",
        "\n",
        "class Adafactor(torch.optim.Optimizer):\n",
        "    \"\"\"Implements Adafactor algorithm.\n",
        "    This implementation is based on:\n",
        "    `Adafactor: Adaptive Learning Rates with Sublinear Memory Cost`\n",
        "    (see https://arxiv.org/abs/1804.04235)\n",
        "    Note that this optimizer internally adjusts the learning rate\n",
        "    depending on the *scale_parameter*, *relative_step* and\n",
        "    *warmup_init* options. To use a manual (external) learning rate\n",
        "    schedule you should set `scale_parameter=False` and\n",
        "    `relative_step=False`.\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): external learning rate (default: None)\n",
        "        eps (tuple[float, float]): regularization constans for square gradient\n",
        "            and parameter scale respectively (default: (1e-30, 1e-3))\n",
        "        clip_threshold (float): threshold of root mean square of\n",
        "            final gradient update (default: 1.0)\n",
        "        decay_rate (float): coefficient used to compute running averages of square\n",
        "            gradient (default: -0.8)\n",
        "        beta1 (float): coefficient used for computing running averages of gradient\n",
        "            (default: None)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        scale_parameter (bool): if True, learning rate is scaled by root mean square of\n",
        "            parameter (default: True)\n",
        "        relative_step (bool): if True, time-dependent learning rate is computed\n",
        "            instead of external learning rate (default: True)\n",
        "        warmup_init (bool): time-dependent learning rate computation depends on\n",
        "            whether warm-up initialization is being used (default: False)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=None, eps=(1e-30, 1e-3), clip_threshold=1.0,\n",
        "                 decay_rate=-0.8, beta1=None, weight_decay=0.0, scale_parameter=True,\n",
        "                 relative_step=True, warmup_init=False):\n",
        "        if lr is not None and relative_step:\n",
        "            raise ValueError('Cannot combine manual lr and relative_step options')\n",
        "        if warmup_init and not relative_step:\n",
        "            raise ValueError('warmup_init requires relative_step=True')\n",
        "\n",
        "        defaults = dict(lr=lr, eps=eps, clip_threshold=clip_threshold, decay_rate=decay_rate,\n",
        "                        beta1=beta1, weight_decay=weight_decay, scale_parameter=scale_parameter,\n",
        "                        relative_step=relative_step, warmup_init=warmup_init)\n",
        "        super(Adafactor, self).__init__(params, defaults)\n",
        "\n",
        "    @property\n",
        "    def supports_memory_efficient_fp16(self):\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def supports_flat_params(self):\n",
        "        return False\n",
        "\n",
        "    def _get_lr(self, param_group, param_state):\n",
        "        rel_step_sz = param_group['lr']\n",
        "        if param_group['relative_step']:\n",
        "            min_step = 1e-6 * param_state['step'] if param_group['warmup_init'] else 1e-2\n",
        "            rel_step_sz = min(min_step, 1.0/math.sqrt(param_state['step']))\n",
        "        param_scale = 1.0\n",
        "        if param_group['scale_parameter']:\n",
        "            param_scale = max(param_group['eps'][1], param_state['RMS'])\n",
        "        return param_scale * rel_step_sz\n",
        "\n",
        "    def _get_options(self, param_group, param_shape):\n",
        "        factored = len(param_shape) >= 2\n",
        "        use_first_moment = param_group['beta1'] is not None\n",
        "        return factored, use_first_moment\n",
        "\n",
        "    def _rms(self, tensor):\n",
        "        return tensor.norm(2) / (tensor.numel() ** 0.5)\n",
        "\n",
        "    def _approx_sq_grad(self, exp_avg_sq_row, exp_avg_sq_col):\n",
        "        r_factor = (\n",
        "            exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True)\n",
        "        ).rsqrt_()\n",
        "        c_factor = exp_avg_sq_col.rsqrt()\n",
        "        return torch.mm(r_factor.unsqueeze(-1), c_factor.unsqueeze(0))\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.dtype in {torch.float16, torch.bfloat16}:\n",
        "                    grad = grad.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adafactor does not support sparse gradients.')\n",
        "\n",
        "                state = self.state[p]\n",
        "                grad_shape = grad.shape\n",
        "\n",
        "                factored, use_first_moment = self._get_options(group, grad_shape)\n",
        "                # State Initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "\n",
        "                    if use_first_moment:\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state['exp_avg'] = torch.zeros_like(grad)\n",
        "                    if factored:\n",
        "                        state['exp_avg_sq_row'] = torch.zeros(grad_shape[:-1]).to(grad)\n",
        "                        state['exp_avg_sq_col'] = torch.zeros(grad_shape[:-2] + grad_shape[-1:]).to(grad)\n",
        "                    else:\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(grad)\n",
        "\n",
        "                    state['RMS'] = 0\n",
        "                else:\n",
        "                    if use_first_moment:\n",
        "                        state['exp_avg'] = state['exp_avg'].to(grad)\n",
        "                    if factored:\n",
        "                        state['exp_avg_sq_row'] = state['exp_avg_sq_row'].to(grad)\n",
        "                        state['exp_avg_sq_col'] = state['exp_avg_sq_col'].to(grad)\n",
        "                    else:\n",
        "                        state['exp_avg_sq'] = state['exp_avg_sq'].to(grad)\n",
        "\n",
        "                p_data_fp32 = p.data\n",
        "                if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
        "                    p_data_fp32 = p_data_fp32.float()\n",
        "\n",
        "                state['step'] += 1\n",
        "                state['RMS'] = self._rms(p_data_fp32)\n",
        "                group['lr'] = self._get_lr(group, state)\n",
        "\n",
        "                beta2t = 1.0 - math.pow(state['step'], group['decay_rate'])\n",
        "                update = (grad**2) + group['eps'][0]\n",
        "                if factored:\n",
        "                    exp_avg_sq_row = state['exp_avg_sq_row']\n",
        "                    exp_avg_sq_col = state['exp_avg_sq_col']\n",
        "\n",
        "                    exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n",
        "                    exp_avg_sq_col.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-2))\n",
        "\n",
        "                    # Approximation of exponential moving average of square of gradient\n",
        "                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)\n",
        "                    update.mul_(grad)\n",
        "                else:\n",
        "                    exp_avg_sq = state['exp_avg_sq']\n",
        "\n",
        "                    exp_avg_sq.mul_(beta2t).add_(1.0 - beta2t, update)\n",
        "                    update = exp_avg_sq.rsqrt().mul_(grad)\n",
        "\n",
        "                update.div_(\n",
        "                    (self._rms(update) / group['clip_threshold']).clamp_(min=1.0)\n",
        "                )\n",
        "                update.mul_(group['lr'])\n",
        "\n",
        "                if use_first_moment:\n",
        "                    exp_avg = state['exp_avg']\n",
        "                    exp_avg.mul_(group['beta1']).add_(1 - group['beta1'], update)\n",
        "                    update = exp_avg\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                p_data_fp32.add_(-update)\n",
        "\n",
        "                if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5EhhEz4Ba3Q",
        "colab_type": "text"
      },
      "source": [
        "# P2Q\n",
        "## Training\n",
        "- Setup config\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmfP76r6f-by",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7a943508-fd72-4d7a-c9ba-e61fd3b38242"
      },
      "source": [
        "dataset = MyDataset(DATA_DIR, 'p2q-train-loader.pkl')\n",
        "dataset_valid = MyDataset(DATA_DIR, 'p2q-valid-loader.pkl')\n",
        "# cach the dataset, so we can load it directly for training\n",
        "\n",
        "#torch.save(dataset, 'train_data.pt')\n",
        "#torch.save(dataset_valid, 'valid_data.pt')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Loaded.\n",
            "Data Loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOz3l_y-r7jE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def mp_fn(index, isTrain):\n",
        "\n",
        "    def sent_dump(data_dir, file_output, samples):\n",
        "        with open(data_dir+'/results/'+file_output, 'w') as file:\n",
        "            file.writelines(samples)\n",
        "\n",
        "    def set_seed(seed=1234):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.manual_seed(seed)\n",
        "    \n",
        "    def collate_fn_t5(batch):\n",
        "        ## TODO: Make the dataset in tensor form \n",
        "        src = torch.tensor([src for (src, tgt) in batch])\n",
        "        tgt = torch.tensor([tgt for (src, tgt) in batch])\n",
        "        #tgt[tgt[:, :]==0] = -100\n",
        "        return src, tgt\n",
        "    \n",
        "    def collate_fn_t5_valid(batch):\n",
        "        ## TODO: Make the dataset in tensor form \n",
        "        src = torch.tensor([src for (src, tgt) in batch])\n",
        "        tgt = torch.tensor([tgt for (src, tgt) in batch])\n",
        "        #tgt[tgt[:, :]==0] = -100\n",
        "        return src, tgt\n",
        "\n",
        "    ## [DEVICE]\n",
        "    set_seed() \n",
        "    device = xm.xla_device()\n",
        "\n",
        "    ## [DATA] Load datasets \n",
        "    xm.master_print(\"Preparing datasets....\")\n",
        "\n",
        "    ## [DATA] Distributed sampler and dataloader\n",
        "    sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True)\n",
        "    \n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset, \n",
        "        batch_size=BATCH_SIZE//2, \n",
        "        sampler=sampler,\n",
        "        num_workers=8,\n",
        "        collate_fn=collate_fn_t5,\n",
        "        drop_last=False)\n",
        "    \n",
        "    ## [DATA] Distributed sampler and dataloader for VALIDATION\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset_valid,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False)\n",
        "    \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        dataset_valid, \n",
        "        batch_size=BATCH_SIZE//2, \n",
        "        sampler=valid_sampler,\n",
        "        num_workers=8,\n",
        "        collate_fn=collate_fn_t5,\n",
        "        drop_last=True)\n",
        "\n",
        "    ## [MODEL] Setup T5\n",
        "    xm.master_print(\"Preparing model....\")\n",
        "    model = T5_gen(config)\n",
        "    \n",
        "    if isTrain:\n",
        "        ## [MODEL] Setup configuration\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "\n",
        "        ## [OPTIM] setup opitimizer\n",
        "        #optimizer = AdaFactor(model.parameters(), non_constant_decay=True, enable_factorization=True)\n",
        "        optimizer = Adafactor(model.parameters())\n",
        "        #optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "        #optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "        xm.master_print(\"Ready to train....\")\n",
        "        # [START]\n",
        "        for epoch in range(5):\n",
        "            xm.master_print(f\"Training Epoch.... {epoch}\")     \n",
        "            start_time = time.time()\n",
        "\n",
        "            # Set to multiprocessing\n",
        "            para_loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n",
        "            tracker = xm.RateTracker()\n",
        "\n",
        "            # [TRAIN]\n",
        "            for i, batch in enumerate(para_loader):\n",
        "                \n",
        "                src, tgt = batch\n",
        "                logits, samples, loss = model(src, tgt)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                xm.optimizer_step(optimizer)\n",
        "\n",
        "                if i & 1000 == 0:\n",
        "                    if xm.is_master_ordinal():\n",
        "                        batch_sent = []\n",
        "                        for sent in samples.detach().cpu().numpy():\n",
        "                            s = T5tokenizer.decode(sent, skip_special_tokens=True)\n",
        "                            batch_sent.append(s)\n",
        "                        xm.master_print(batch_sent)\n",
        "\n",
        "                if i % 100 == 0:\n",
        "                    xm.master_print('[TRAIN-P2Q] steps: {}/{}'.format(i, len(para_loader)))\n",
        "                    xm.master_print('[{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}\\n'.format(\n",
        "                        device, i, (loss/src.size(0)).item(), tracker.rate(), tracker.global_rate(), time.asctime()))\n",
        "                \n",
        "    else:\n",
        "\n",
        "        ## [MODEL] Setup configuration\n",
        "        model.to(device)\n",
        "\n",
        "        xm.master_print(\"Ready to validation....\")\n",
        "        # [START]\n",
        "        for epoch in range(5):\n",
        "            xm.master_print(f\"Validation Epoch.... {epoch}\")     \n",
        "            start_time = time.time()\n",
        "\n",
        "            # Set to multiprocessing\n",
        "            batch_sent = {'pred': list(), 'truth': list()}\n",
        "            para_loader = pl.ParallelLoader(valid_loader, [device]).per_device_loader(device)\n",
        "            tracker = xm.RateTracker()\n",
        "\n",
        "            for i, batch in enumerate(para_loader):\n",
        "                src, tgt = batch\n",
        "                #logits, samples, loss = model(src, tgt)\n",
        "                #_, samples = model.inference(src, device=device)\n",
        "                #_, samples = model.inference(src)\n",
        "                model.t5.generate(src)\n",
        "                loss=0\n",
        "\n",
        "                # Recording\n",
        "                pred_collect, truth_collect = [], []\n",
        "                for sent_valid in samples.detach().cpu().numpy():\n",
        "                    s = T5tokenizer.decode(sent_valid, skip_special_tokens=True)\n",
        "                    pred_collect.append(s)\n",
        "                    \n",
        "                for sent2_valid in tgt.detach().cpu().numpy():\n",
        "                    s2 = T5tokenizer.decode(sent2_valid, skip_special_tokens=True)\n",
        "                    truth_collect.append(s2)\n",
        "                    \n",
        "                batch_sent['pred'] += [sentence + '\\n' for sentence in pred_collect]\n",
        "                batch_sent['truth'] += [sentence + '\\n' for sentence in truth_collect]\n",
        "\n",
        "                if i & 200 == 0:\n",
        "                    if xm.is_master_ordinal():\n",
        "                        xm.master_print(pred_collect)\n",
        "                        xm.master_print(truth_collect)\n",
        "\n",
        "                if i % 100 == 0:\n",
        "                    xm.master_print('[VALID-P2Q] steps: {}/{}'.format(i, len(para_loader)))\n",
        "                    xm.master_print('[{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}\\n'.format(\n",
        "                        device, i, (loss/src.size(0)).item(), tracker.rate(), tracker.global_rate(), time.asctime()))\n",
        "                \n",
        "            sent_dump(DATA_DIR, 'singletask/p2q-predict-tpu-E%i.txt'%epoch, batch_sent['pred'])\n",
        "            xm.master_print('Prediction saved!!: %i'%epoch)\n",
        "            sent_dump(DATA_DIR, 'singletask/p2q-target-tpu-E%i.txt'%epoch, batch_sent['truth'])\n",
        "            \n",
        "            # Epoch finised!\n",
        "            print(\"Process\", index, \"finished validation. Validated time was:\", time.time() - start_time) \n",
        "\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G3StNfLTF62",
        "colab_type": "code",
        "outputId": "a3e0b310-2e8a-4909-9742-41ab35f376cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "xmp.spawn(mp_fn, args=(False, ), nprocs=8, start_method='fork')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing datasets....\n",
            "Preparing model....\n",
            "Ready to validation....\n",
            "Validation Epoch.... 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-97da63e41094>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     76\u001b[0m         ready = multiprocessing.connection.wait(\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS3O8JB21YE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}